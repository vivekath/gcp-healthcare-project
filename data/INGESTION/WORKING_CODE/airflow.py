# 6Ô∏è‚É£ Environment Variables is okay, but you suggested we can add as variable in Airflow UI

‚úÖ Recommended Setup (Best Practice)
In Composer ‚Üí Admin ‚Üí Variables
GCS_BUCKET = heathcare-bucket-12112025
BQ_PROJECT = quantum-episode-345713
HOSPITAL_NAME = hospital-a

In DAGs
from airflow.models import Variable

GCS_BUCKET = Variable.get("GCS_BUCKET")
BQ_PROJECT = Variable.get("BQ_PROJECT")
HOSPITAL_NAME = Variable.get("HOSPITAL_NAME")

In Composer ‚Üí Environment Variables
ENV = prod
GCP_REGION = us-east1

üéØ Final Recommendation for Your Case

Since:

You want flexibility

You are orchestrating multiple jobs

Values may change per environment

üëâ Use Airflow Variables, not Environment Variables
üëâ Environment Variables only for platform-level constants


# -------------------------------------------
# Provide a dev / qa / prod variable strategy
Below is a clean, production-grade dev / qa / prod variable strategy for Cloud Composer (Airflow) that scales well and avoids hard-coding.

No fluff, this is how it‚Äôs done in real projects.

‚úÖ Dev / QA / Prod Variable Strategy (Composer / Airflow)
1Ô∏è‚É£ Single DAG, Multiple Environments (Golden Rule)

Same DAG code runs in:

dev

qa

prod

Only variables change, not code.

2Ô∏è‚É£ Define Environment Identifier (ONE place only)
Composer ‚Üí Environment Variables
ENV = dev     # or qa / prod


Access in DAG:

import os
ENV = os.getenv("ENV", "dev")

3Ô∏è‚É£ Variable Naming Convention (Critical)

Prefix every Airflow Variable with environment.

dev_GCS_BUCKET
qa_GCS_BUCKET
prod_GCS_BUCKET

dev_BQ_PROJECT
qa_BQ_PROJECT
prod_BQ_PROJECT


This prevents cross-environment leaks.

4Ô∏è‚É£ Airflow Variables (Admin ‚Üí Variables)
DEV
dev_GCS_BUCKET = heathcare-dev-bucket
dev_BQ_PROJECT = quantum-episode-dev
dev_HOSPITAL_NAME = hospital-a

QA
qa_GCS_BUCKET = heathcare-qa-bucket
qa_BQ_PROJECT = quantum-episode-qa
qa_HOSPITAL_NAME = hospital-a

PROD
prod_GCS_BUCKET = heathcare-prod-bucket
prod_BQ_PROJECT = quantum-episode-prod
prod_HOSPITAL_NAME = hospital-a

5Ô∏è‚É£ Central Variable Loader (Best Practice)

Create one helper function (inside DAG or utils.py):

from airflow.models import Variable
import os

ENV = os.getenv("ENV", "dev")

def get_var(key: str):
    return Variable.get(f"{ENV}_{key}")

6Ô∏è‚É£ Use Variables Everywhere (Clean DAG Code)
GCS_BUCKET = get_var("GCS_BUCKET")
BQ_PROJECT = get_var("BQ_PROJECT")
HOSPITAL_NAME = get_var("HOSPITAL_NAME")


No if env == dev logic anywhere else.

7Ô∏è‚É£ Dataproc Job Config Example
GCS_JOB_FILE = f"gs://{GCS_BUCKET}/data/INGESTION/hospitalA_mysqlToLanding.py"


Works automatically across environments.

8Ô∏è‚É£ BigQuery Usage Example
BQ_AUDIT_TABLE = f"{BQ_PROJECT}.temp_dataset.audit_log"

9Ô∏è‚É£ What Goes WHERE (Very Important)
Item	Where
Project IDs	Airflow Variables
Bucket names	Airflow Variables
Dataset names	Airflow Variables
ENV flag	Composer Env Var
Secrets (DB pwd)	Secret Manager
Machine types	DAG constants
# ---------------------------------------
üîê Secrets Strategy (Do NOT skip)
Secret Manager
dev_mysql_password
qa_mysql_password
prod_mysql_password


Access:

from google.cloud import secretmanager

def get_secret(secret_id):
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{BQ_PROJECT}/secrets/{ENV}_{secret_id}/versions/latest"
    return client.access_secret_version(name=name).payload.data.decode()

10Ô∏è‚É£ Folder Structure (Recommended)
dags/
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îî‚îÄ‚îÄ hospital_dag.py
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ variables.py
‚îÇ   ‚îú‚îÄ‚îÄ secrets.py
‚îÇ   ‚îî‚îÄ‚îÄ dataproc.py

11Ô∏è‚É£ Promotion Flow (Dev ‚Üí QA ‚Üí Prod)
Step	Action
Dev	Add dev_* variables
QA	Copy & rename to qa_*
Prod	Copy & rename to prod_*
Code	NO CHANGE
üéØ Final Best-Practice Summary

‚úÖ One DAG
‚úÖ One ENV variable
‚úÖ Env-prefixed Airflow Variables
‚úÖ Central variable loader
‚úÖ Secrets in Secret Manager
‚úÖ Zero hard-coding